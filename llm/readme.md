Ğ§Ğ°ÑÑ‚ÑŒ, Ğ¾Ñ‚Ğ½Ğ¾ÑÑÑ‰Ğ°ÑÑÑ Ğº LLM Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾

# ğŸ§  Large Language Models (LLM) Module

ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ†Ğ¸ĞºĞ»Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ğ¿Ğ¾Ğº Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ **Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ** Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ LLM.

## ğŸ“‚ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ

Ğ’ÑĞµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ (ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, ĞºĞ¾Ğ´, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ) Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ `papers/` Ğ¸ ÑĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¿Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ:

### ğŸ”¹ I. Ğ’Ñ…Ğ¾Ğ´ Ğ¸ Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚

* **`00_The_Guidebook_&_Roadmap`**: Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ, ÑĞºĞ¸Ğ»Ğ»ÑĞµÑ‚Ñ‹ (Anthropic, HF). Ğ§Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°.
* **`02_Model_Backbones`**: "Ğ–ĞµĞ»ĞµĞ·Ğ¾" Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.
  * *Transformer Evolution:* ĞšĞ»Ğ°ÑÑĞ¸ĞºĞ° (GPT, Llama).
  * *Post-Transformer:* ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ ÑƒĞ±Ğ¸Ñ‚ÑŒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ (Mamba, RWKV, Linear Attention).

### ğŸ”¹ II. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ (ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ»Ğ¾Ğ¹)

* **`01_Data_Centric_AI`**: Ğ¡Ğ°Ğ¼Ñ‹Ğ¹ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ».
  * *Pretraining:* ĞĞ° Ñ‡ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ğ·Ñƒ (FineWeb, The Pile).
  * *Synthetic:* ĞšĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Cosmopedia, Magpie).
  * *Filtering:* ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ.
  * *Instruction Tuning:* ĞšĞ¾Ğ´ Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ (UltraChat) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ "Ğ´Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ»ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°" Ğ² Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ°.

### ğŸ”¹ III. ĞœÑ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ ĞĞ³ĞµĞ½Ñ‚Ñ‹ (SOTA)

* **`03_Reasoning_Architectures`**: System 2 Thinking. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ "Ğ´ÑƒĞ¼Ğ°ÑÑ‚" (CoT, ToT, DeepSeek-R1).
* **`04_Agentic_Systems_&_MCP`**: Ğ’Ñ‹Ñ…Ğ¾Ğ´ Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ¼Ğ¸Ñ€. ĞŸÑ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ (MCP), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Toolformer), Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹.

### ğŸ”¹ IV. Ğ˜Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ

* **`05_Memory_&_Context`**: RAG, Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (RoPE, Linear Biases).
* **`06_Engineering_&_Tuning`**: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (LoRA, QLoRA, Quantization).
* **`07_Benchmarks_&_Evals`**: ĞšĞ°Ğº Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ (LLM-as-a-Judge, Arena).

---

ğŸ‘‰ **Ğ¡ Ñ‡ĞµĞ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ?** ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸Ñ‚Ğµ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ [`HOW_TO_READ.md`](./HOW_TO_READ.md) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.

---

Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ°:

```md
llm_useful_info/
â”œâ”€â”€ llm
â”‚   â”œâ”€â”€ papers
â”‚   â”‚   â”œâ”€â”€ 00_The_Guidebook_&_Roadmap
â”‚   â”‚   â”‚   â”œâ”€â”€ [claude-scientific-skills](https://github.com/K-Dense-AI/claude-scientific-skills)
â”‚   â”‚   â”‚   â”œâ”€â”€ AI Engineering Guidebook.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ hf-skills-training.md
â”‚   â”‚   â”‚   â”œâ”€â”€ README_Reading_Order.md
â”‚   â”‚   â”‚   â”œâ”€â”€ the-smol-training-playbook-the-secrets-to-building-world-class-llms.pdf
â”‚   â”‚   â”‚   â””â”€â”€ UAF_blog_post_links.md
â”‚   â”‚   â”œâ”€â”€ 01_Data_Centric_AI
â”‚   â”‚   â”‚   â”œâ”€â”€ 01_Pretraining_Corpora
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Colossal Clean Crawled Corpus.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ The FineWeb Datasets.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ The Pile.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ The RefinedWeb Dataset.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ 02_Data_Filtering_Quality
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Deduplicating Training Data Makes Language Models Better.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Dolma.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Quality is All You Need.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ 03_Synthetic_Data
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cosmopedia.md
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Magpie.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Textbooks Are All You Need II.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Textbooks Are All You Need.pdf
â”‚   â”‚   â”‚   â””â”€â”€ 04_Instruction_Tuning_Data
â”‚   â”‚   â”‚       â”œâ”€â”€ UltraChat
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ split_long.py
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ tmp.py
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ ultra_eval.json
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ vllm_chatloop
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ figures
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ alpaca.png
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ compare_ultra.jpg
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ figure.png
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ meta_topic.png
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ ultra-process.png
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ ultra_logo.png
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ wizard_test.jpg
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ paper
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ UltraFuser-paper.pdf
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ train_legacy
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ template
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â”‚   â””â”€â”€ template.txt
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â””â”€â”€ ultrachat_dataset.py
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ train_bm.py
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ ultrachat_dataset.py
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ UltraLM
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ util
â”‚   â”‚   â”‚       â”‚   â”‚   â”‚   â””â”€â”€ inference.py
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ chat_cli.sh
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ inference_cli.py
â”‚   â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ recover.sh
â”‚   â”‚   â”‚       â”‚   â”‚   â””â”€â”€ weight_diff.py
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ LICENSE
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ README.md
â”‚   â”‚   â”‚       â”œâ”€â”€ Orca Progressive Learning.pdf
â”‚   â”‚   â”‚       â””â”€â”€ Self-Instruct.pdf
â”‚   â”‚   â”œâ”€â”€ 02_Model_Backbones
â”‚   â”‚   â”‚   â”œâ”€â”€ 01_Transformer_Evolution & foundations core
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Attention is all you need.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Gemini 1.5 Unlocking multimodal understanding.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gpt-1.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ GPT-4.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Language Models are Few-Shot Learners.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Large Language Models A Survey.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Mixtral of Experts.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Scaling Laws for Neural Language Models.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Switch Transformers Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ The Llama 3 Herd of Models.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Training Compute-Optimal Large Language Models.pdf
â”‚   â”‚   â”‚   â””â”€â”€ 02_Post_Transformer_&_SSM
â”‚   â”‚   â”‚       â”œâ”€â”€ 01_Origins_Linear_Attn
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Attention Free Transformer.pdf
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Linear Attention Mechanism.pdf
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ RWKV Reinventing RNNs for the Transformer Era.pdf
â”‚   â”‚   â”‚       â”œâ”€â”€ 02_State_Space_Models
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Efficiently Modeling Long Sequences with Structured State Spaces.pdf
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Mamba-1. Linear-Time Sequence Modeling with Selective State Spaces.pdf
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Mamba-3 Technical Report.pdf
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ Mamba-3D.pdf
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ Transformers are SSMs (Mamba-2).pdf
â”‚   â”‚   â”‚       â””â”€â”€ 03_Hybrid_Architectures
â”‚   â”‚   â”‚           â”œâ”€â”€ Griffin Mixing Gated Linear Recurrences with Local Attention.pdf
â”‚   â”‚   â”‚           â”œâ”€â”€ Jamba A Hybrid Transformer-Mamba Language Model.pdf
â”‚   â”‚   â”‚           â”œâ”€â”€ RecurrentGemma.pdf
â”‚   â”‚   â”‚           â””â”€â”€ Zamba A Compact 7B SSM-Transformer Hybrid.pdf
â”‚   â”‚   â”œâ”€â”€ 03_Reasoning_Architectures
â”‚   â”‚   â”‚   â”œâ”€â”€ 01_Hierarchical_Reasoning
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Hierarchical Reasoning Model.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Quiet-STaR.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Towards System 2 Reasoning in LLMs.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ 02_Reinforcement_Thinking
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DeepSeek-R1.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Graph of Thoughts Solving Elaborate Problems with Large Language Models.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Letâ€™s Verify Step by Step.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Manifold-Constrained Hyper-Connections.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Model-First Reasoning LLM Agents.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RL_base.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Tree of Thoughts Deliberate Problem Solving.pdf
â”‚   â”‚   â”‚   â””â”€â”€ 03_Alignment_History
â”‚   â”‚   â”‚       â”œâ”€â”€ Constitutional AI.pdf
â”‚   â”‚   â”‚       â”œâ”€â”€ DPO.pdf
â”‚   â”‚   â”‚       â””â”€â”€ RLHF.pdf
â”‚   â”‚   â”œâ”€â”€ 04_Agentic_Systems_&_MCP
â”‚   â”‚   â”‚   â”œâ”€â”€ 01_Protocols
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ MCP.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ 02_Agentic_Frameworks
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ OPENHANDS.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SWE-agent Agent-Computer Interfaces.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Toolformer Language Models Can Teach Themselves to Use Tools.pdf
â”‚   â”‚   â”‚   â””â”€â”€ 03_Environment
â”‚   â”‚   â”‚       â””â”€â”€ VOYAGER.pdf
â”‚   â”‚   â”œâ”€â”€ 05_Memory_&_Context
â”‚   â”‚   â”‚   â”œâ”€â”€ LEANN-main.zip
â”‚   â”‚   â”‚   â”œâ”€â”€ LEANN.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ Retrieval-Augmented Generation.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ RoPE.pdf
â”‚   â”‚   â”‚   â””â”€â”€ TRAIN SHORT, TEST LONG ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION.pdf
â”‚   â”‚   â”œâ”€â”€ 06_Engineering_&_Tuning
â”‚   â”‚   â”‚   â”œâ”€â”€ finLORA.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ LoRA.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ MEA.zip
â”‚   â”‚   â”‚   â”œâ”€â”€ QLORA Efficient Finetuning of Quantized LLMs.pdf
â”‚   â”‚   â”‚   â””â”€â”€ Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° LLM Ñ Ğ½ÑƒĞ»Ñ.rar
â”‚   â”‚   â”œâ”€â”€ 07_Benchmarks_&_Evals
â”‚   â”‚   â”‚   â”œâ”€â”€ ARC-AGI.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ Beyond the Imitation Game.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ Chatbot Arena An Open Platform for Evaluating LLMs by Human Preference.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ LLM-as-a-Judge.pdf
â”‚   â”‚   â”‚   â””â”€â”€ SWE-BENCH CAN LANGUAGE MODELS RESOLVE real-world github issues.pdf
â”‚   â”‚   â”œâ”€â”€ xx_prompting
â”‚   â”‚   â”‚   â”œâ”€â”€ Chain-of-Thought Prompting Elicits Reasoning.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ REACT SYNERGIZING REASONING AND ACTING IN LM.pdf
â”‚   â”‚   â”‚   â””â”€â”€ The Prompt Report A Systematic Survey of Prompt Engineering.pdf
â”‚   â”‚   â””â”€â”€ readme.md
â”‚   â”œâ”€â”€ Supporting_Resources
â”‚   â”‚   â”œâ”€â”€ External_Book_CS249R
â”‚   â”‚   â”‚   â”œâ”€â”€ [cs249r_book](cs249r_book)
â”‚   â”‚   â”‚   â””â”€â”€ book_link.md
â”‚   â”‚   â”œâ”€â”€ LEANN_Repo_Link.md
â”‚   â”‚   â”œâ”€â”€ Mamba_SSM_Repo_Link.md
â”‚   â”‚   â”œâ”€â”€ Stanford_CME295_link.md
â”‚   â”‚   â””â”€â”€ VLA_blogpost_link.md
â”‚   â””â”€â”€ readme.md
â”œâ”€â”€ PROJECT_STRUCTURE.md
â””â”€â”€ README.md

```
